{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import numpy as np\n",
        "import json\n",
        "# import torch_xla.core.xla_model as xm\n",
        "from torch.nn.modules.pooling import MaxPool2d\n",
        "\n",
        "N = 11\n",
        "inputc = 3\n",
        "batch = 4\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self,ic,oc,pad=1):\n",
        "    super().__init__()\n",
        "    self.conv=nn.Conv2d(\n",
        "        ic,oc,3,stride=1,padding=pad,dilation=1,\n",
        "        groups=1,bias=False,padding_mode='zeros'\n",
        "    )\n",
        "    self.bn = nn.BatchNorm2d(oc)\n",
        "    # 批量归一化\n",
        "  def forward(self,x):\n",
        "    y = self.conv(x)\n",
        "    y = self.bn(y)\n",
        "    y = torch.relu(y)\n",
        "    # relu激活函数\n",
        "    return y\n",
        "\n",
        "class RSN(nn.Module):\n",
        "  def __init__(self,ic,oc):\n",
        "      super().__init__()\n",
        "      self.conv_net = nn.Sequential(\n",
        "          CNN(ic,oc),\n",
        "          CNN(oc,ic)\n",
        "      )\n",
        "  def forward(self,x):\n",
        "    x = self.conv_net(x) + x\n",
        "    return x\n",
        "\n",
        "class outputhdv1(nn.Module):\n",
        "  def __init__(self,oc,hmc):\n",
        "    super().__init__()\n",
        "    self.cnn=CNN(oc,hmc)\n",
        "    self.pL = nn.Conv2d(hmc,1,1)\n",
        "  def forward(self,h):\n",
        "    x = self.cnn(h)\n",
        "    x = self.pL(x)\n",
        "    x = x.squeeze(1)\n",
        "    return x\n",
        "\n",
        "class outputhdv2(nn.Module):\n",
        "  def __init__(self,oc,hmc):\n",
        "    super().__init__()\n",
        "    self.cnn=CNN(oc,hmc)\n",
        "    self.vL = nn.Conv2d(hmc,1,1)\n",
        "    self.vL2 = nn.Linear(N*N,128)\n",
        "    self.vL3 = nn.Linear(128,1)\n",
        "    self.vL4 = nn.Sigmoid()\n",
        "  def forward(self,h):\n",
        "    x = self.cnn(h)\n",
        "    x = self.vL(x)\n",
        "    x = x.squeeze(1)\n",
        "    x = x.view(-1,N*N)\n",
        "\n",
        "    x = self.vL2(x)\n",
        "    x = torch.relu(x)\n",
        "    x = self.vL3(x)\n",
        "    \n",
        "    x = self.vL4(x)\n",
        "    x = x.squeeze(1)\n",
        "    # z = torch.zeros((batch,2),dtype=float,device=device)\n",
        "    # for i in range(batch):\n",
        "    #   z[i] = self.vL2(x[i].reshape(-1))\n",
        "    # print(x.device,x.shape)\n",
        "    return x\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self,b,f):\n",
        "    super().__init__()\n",
        "    self.name = \"res\"\n",
        "    self.size = (b,f)\n",
        "    self.iph = CNN(inputc,f*2,pad = 2)\n",
        "    self.pool1 = nn.MaxPool2d(3,stride = 1,padding = 0)\n",
        "    self.tru = nn.ModuleList()\n",
        "    self.tru.append(CNN(f*2,f))\n",
        "    for i in range(b):\n",
        "      self.tru.append(RSN(f,f))\n",
        "    self.opt1 = outputhdv1(f,f)\n",
        "    self.opt2 = outputhdv2(f,f)\n",
        "\n",
        "  def forward(self,x):\n",
        "    if(x.shape[1]==2):\n",
        "      x = torch.cat((x,torch.zeros(batch,inputc-2,N,N,device=device)),dim=1)\n",
        "    # print(x.shape)\n",
        "    # nn.exit()\n",
        "    h = self.iph(x)\n",
        "    h = self.pool1(h)\n",
        "    for blk in self.tru :\n",
        "      h = blk(h)\n",
        "    h = self.opt2(h)\n",
        "    return h\n",
        "\n",
        "ModelDic = {\n",
        "    \"res\":model\n",
        "}\n",
        "\n",
        "class bd:\n",
        "  def __init__(self):\n",
        "    self.board = np.zeros(shape=(2,N,N))\n",
        "  def transpose(self):\n",
        "    self.board = self.board.swapaxes(1,2)\n",
        "  def play(self,x,y,color):\n",
        "    self.board[color,y,x]=1\n",
        "  def isL(self,x,y):\n",
        "    if(self.board[0,y,x] or self.board[1,y,x]):\n",
        "      return False\n",
        "    return True\n",
        "\n",
        "def loadM(filep):\n",
        "  if(filep=='stochastic'):\n",
        "    return 'stochastic'\n",
        "  model = torch.load(filep,map_location=torch.device('cpu'))\n",
        "  return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8wuFyNdk0IO",
        "outputId": "377ef7bb-e11d-4911-856d-682a2c86f8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pYyKkFwY1Xi"
      },
      "source": [
        "上面部分是模型的建模\n",
        "\n",
        "下面部分是模型的训练"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 写自定义dataset类\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "N = 11\n",
        "T = 1\n",
        "Nt = 500\n",
        "Ntrc = 10\n",
        "\n",
        "class Mydataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,filep):\n",
        "        \n",
        "        file = open(filep,\"r\")\n",
        "        rcm = file.readlines()\n",
        "        L = len(rcm)\n",
        "        i = 0\n",
        "        self.mrc = []\n",
        "\n",
        "        # L = min(L,25000)\n",
        "\n",
        "        while i < L:\n",
        "            board=bd()\n",
        "            p = int(rcm[i])\n",
        "            sm = rcm[i+1].split()\n",
        "            k = 0\n",
        "            win = float(rcm[i+2])\n",
        "            if p>=50 and p<=170: \n",
        "              #去掉部分极端数据 \n",
        "              while k < p:\n",
        "                  x = int(sm[k])\n",
        "                  y = int(sm[k+1])\n",
        "                  o = ((k//2) & 1)\n",
        "                  if (k>0.7*p and random.randint(0,99)>69):\n",
        "                    # 学习中局策略\n",
        "                      a = copy.deepcopy(board.board)\n",
        "                      a = a.astype(np.float32)\n",
        "                      # if o == 0:\n",
        "                      #   c = np.array(0,dtype=float)\n",
        "                      # else :\n",
        "                      #   c = np.array(1,dtype=float)  \n",
        "                      # if o == 1:\n",
        "                      #     b = np.asarray([a[1],a[0]])\n",
        "                      #     a = b.swapaxes(1,2)\n",
        "                      #     c = np.array([0,1],dtype=float)\n",
        "                      # a=np.insert(a,2,a[0]-a[1],axis=0)\n",
        "                      # md我是智障吧我直接把胜者信息透露给他了\n",
        "                      self.mrc.append((a,win))\n",
        "                  board.play(x,y,o)\n",
        "                  k += 2\n",
        "            i += 3\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.mrc[index]\n",
        "    def __len__(self):\n",
        "        return len(self.mrc)\n",
        "\n",
        "def modeltest(model):\n",
        "    model.eval()\n",
        "    sumloss = 0\n",
        "    for input,label in test_loader:\n",
        "      input = input.to(device)\n",
        "      label = label.to(device)\n",
        "      if(label.size()[0]!=batch):\n",
        "          break      \n",
        "      with torch.no_grad():\n",
        "        outputs = model(input)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      loss = criterion(outputs, label)\n",
        "      sumloss += loss.item()\n",
        "    print(f\"Test Loss is {sumloss/(test_size)*batch:.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # train_data = Mydataset(\"TrainData.in2\")\n",
        "    # test_data = Mydataset(\"TestData.in\")\n",
        "    dataset = Mydataset(\"TrainData.in2\")\n",
        "\n",
        "    # train_size = len(train_data)\n",
        "    # test_size = len(test_data)\n",
        "    train_size = int(0.97 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "\n",
        "    train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size = batch,shuffle = True)\n",
        "    test_loader = DataLoader(test_data, batch_size= batch, shuffle = False)\n",
        "\n",
        "    # mymod = loadM(\"/content/NR1/model_2_55000.pth\")\n",
        "    mymod = model(10,96)\n",
        "    mymod = mymod.to(device)\n",
        "\n",
        "    print(train_size//batch,test_size//batch)\n",
        "    # nn.exit(0)\n",
        "    print(mymod.parameters)\n",
        "    # nn.exit()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(mymod.parameters())\n",
        "    epochn = 6\n",
        "    \n",
        "    for epoch in range(epochn):\n",
        "        runningloss = 0.0\n",
        "        for i,data in enumerate(train_loader,0):\n",
        "            input,label = data\n",
        "            # print(input,label)\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            # print(input.device,label.device)\n",
        "\n",
        "            if(label.size()[0]!=batch):\n",
        "              break\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = mymod(input)\n",
        "            # print(output.device,label.device)\n",
        "\n",
        "            # print(output.shape,output.dtype)\n",
        "            # print(label.shape,label.dtype)\n",
        "            loss = criterion(output,label)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # nn.exit()\n",
        "            runningloss += loss.item()\n",
        "            if(i % Nt == 0 and i != 0):\n",
        "                print('[%d,%5d] loss : %.8f' %\n",
        "                        (epoch+1,i+1,runningloss/Nt))\n",
        "                if(i % (Nt*Ntrc) == 0):\n",
        "                    torch.save(mymod, f\"NR2/model_{epoch}_{i}.pth\")\n",
        "                    modeltest(mymod)\n",
        "                runningloss = 0.0\n",
        "                \n",
        "    print(\"Finished Training\")\n",
        "\n",
        "# 19069"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwNdOX_yqhVv",
        "outputId": "b0840a43-0275-4bf1-b3ed-ef4d8e8d8734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68028 2104\n",
            "<bound method Module.parameters of model(\n",
            "  (iph): CNN(\n",
            "    (conv): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (pool1): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  (tru): ModuleList(\n",
            "    (0): CNN(\n",
            "      (conv): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (4): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (5): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (6): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (7): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (8): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (9): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (10): RSN(\n",
            "      (conv_net): Sequential(\n",
            "        (0): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): CNN(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (opt1): outputhdv1(\n",
            "    (cnn): CNN(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (pL): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (opt2): outputhdv2(\n",
            "    (cnn): CNN(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (vL): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (vL2): Linear(in_features=121, out_features=128, bias=True)\n",
            "    (vL3): Linear(in_features=128, out_features=1, bias=True)\n",
            "    (vL4): Sigmoid()\n",
            "  )\n",
            ")>\n",
            "[1,  501] loss : 2.63120840\n",
            "[1, 1001] loss : 2.44886157\n",
            "[1, 1501] loss : 2.56809818\n",
            "[1, 2001] loss : 2.37745935\n",
            "[1, 2501] loss : 2.49884679\n",
            "[1, 3001] loss : 2.53661132\n",
            "[1, 3501] loss : 2.55620484\n",
            "[1, 4001] loss : 2.56384824\n",
            "[1, 4501] loss : 2.52349643\n",
            "[1, 5001] loss : 2.43620593\n",
            "Test Loss is 2.459979\n",
            "[1, 5501] loss : 2.78309498\n",
            "[1, 6001] loss : 2.71990954\n",
            "[1, 6501] loss : 2.75595320\n",
            "[1, 7001] loss : 2.71990954\n",
            "[1, 7501] loss : 2.83635827\n",
            "[1, 8001] loss : 2.69218366\n",
            "[1, 8501] loss : 2.77258873\n",
            "[1, 9001] loss : 2.83635827\n",
            "[1, 9501] loss : 2.77813391\n",
            "[1,10001] loss : 2.74486284\n",
            "Test Loss is 2.732397\n",
            "[1,10501] loss : 2.75040802\n",
            "[1,11001] loss : 2.75595320\n",
            "[1,11501] loss : 2.81417756\n",
            "[1,12001] loss : 2.69772883\n",
            "[1,12501] loss : 2.75318061\n",
            "[1,13001] loss : 2.82249533\n",
            "[1,13501] loss : 2.74763543\n",
            "[1,14001] loss : 2.79754203\n",
            "[1,14501] loss : 2.81972274\n",
            "[1,15001] loss : 2.77536132\n",
            "Test Loss is 2.732397\n",
            "[1,15501] loss : 2.81417756\n",
            "[1,16001] loss : 2.62009635\n",
            "[1,16501] loss : 2.74763543\n",
            "[1,17001] loss : 2.71436437\n",
            "[1,17501] loss : 2.72545472\n",
            "[1,18001] loss : 2.80031462\n",
            "[1,18501] loss : 2.81972274\n",
            "[1,19001] loss : 2.67832071\n",
            "[1,19501] loss : 2.76704355\n",
            "[1,20001] loss : 2.68663848\n",
            "Test Loss is 2.732397\n",
            "[1,20501] loss : 2.67277554\n",
            "[1,21001] loss : 2.68941107\n",
            "[1,21501] loss : 2.78922426\n",
            "[1,22001] loss : 2.75595320\n",
            "[1,22501] loss : 2.75040802\n",
            "[1,23001] loss : 2.80585979\n",
            "[1,23501] loss : 2.74763543\n",
            "[1,24001] loss : 2.80031462\n",
            "[1,24501] loss : 2.71990954\n",
            "[1,25001] loss : 2.78922426\n",
            "Test Loss is 2.732397\n",
            "[1,25501] loss : 2.70050142\n",
            "[1,26001] loss : 2.82249533\n",
            "[1,26501] loss : 2.66723036\n",
            "[1,27001] loss : 2.76427096\n",
            "[1,27501] loss : 2.76427096\n",
            "[1,28001] loss : 2.68109330\n",
            "[1,28501] loss : 2.72822731\n",
            "[1,29001] loss : 2.67554812\n",
            "[1,29501] loss : 2.81140497\n",
            "[1,30001] loss : 2.90012781\n",
            "Test Loss is 2.732397\n",
            "[1,30501] loss : 2.76981614\n",
            "[1,31001] loss : 2.80585979\n",
            "[1,31501] loss : 2.74763543\n",
            "[1,32001] loss : 2.75595320\n",
            "[1,32501] loss : 2.81972274\n",
            "[1,33001] loss : 2.71990954\n",
            "[1,33501] loss : 2.72545472\n",
            "[1,34001] loss : 2.85853898\n",
            "[1,34501] loss : 2.65891259\n",
            "[1,35001] loss : 2.72545472\n",
            "Test Loss is 2.732397\n",
            "[1,35501] loss : 2.77536132\n",
            "[1,36001] loss : 2.80308721\n",
            "[1,36501] loss : 2.65891259\n",
            "[1,37001] loss : 2.88903746\n",
            "[1,37501] loss : 2.77813391\n",
            "[1,38001] loss : 2.71713696\n",
            "[1,38501] loss : 2.76427096\n",
            "[1,39001] loss : 2.78367908\n",
            "[1,39501] loss : 2.78367908\n",
            "[1,40001] loss : 2.84744863\n",
            "Test Loss is 2.732397\n",
            "[1,40501] loss : 2.61732376\n",
            "[1,41001] loss : 2.76981614\n",
            "[1,41501] loss : 2.73654508\n",
            "[1,42001] loss : 2.69495625\n",
            "[1,42501] loss : 2.70604660\n",
            "[1,43001] loss : 2.82804050\n",
            "[1,43501] loss : 2.90012781\n",
            "[1,44001] loss : 2.69495625\n",
            "[1,44501] loss : 2.69218366\n",
            "[1,45001] loss : 2.76149837\n",
            "Test Loss is 2.732397\n",
            "[1,45501] loss : 2.75595320\n",
            "[1,46001] loss : 2.76704355\n",
            "[1,46501] loss : 2.90567299\n",
            "[1,47001] loss : 2.68109330\n",
            "[1,47501] loss : 2.76704355\n",
            "[1,48001] loss : 2.78645167\n",
            "[1,48501] loss : 2.80863238\n",
            "[1,49001] loss : 2.78367908\n",
            "[1,49501] loss : 2.70050142\n",
            "[1,50001] loss : 2.75872579\n",
            "Test Loss is 2.732397\n",
            "[1,50501] loss : 2.66168518\n",
            "[1,51001] loss : 2.80308721\n",
            "[1,51501] loss : 2.85576639\n",
            "[1,52001] loss : 2.67277554\n",
            "[1,52501] loss : 2.72822731\n",
            "[1,53001] loss : 2.75595320\n",
            "[1,53501] loss : 2.76427096\n",
            "[1,54001] loss : 2.79754203\n",
            "[1,54501] loss : 2.72822731\n",
            "[1,55001] loss : 2.70050142\n",
            "Test Loss is 2.732397\n",
            "[1,55501] loss : 2.72822731\n",
            "[1,56001] loss : 2.68663848\n",
            "[1,56501] loss : 2.76149837\n",
            "[1,57001] loss : 2.69495625\n",
            "[1,57501] loss : 2.83635827\n",
            "[1,58001] loss : 2.68386589\n",
            "[1,58501] loss : 2.78090650\n",
            "[1,59001] loss : 2.76427096\n",
            "[1,59501] loss : 2.73099990\n",
            "[1,60001] loss : 2.78922426\n",
            "Test Loss is 2.732397\n",
            "[1,60501] loss : 2.69495625\n",
            "[1,61001] loss : 2.78922426\n",
            "[1,61501] loss : 2.74763543\n",
            "[1,62001] loss : 2.86131157\n",
            "[1,62501] loss : 2.76149837\n",
            "[1,63001] loss : 2.78090650\n",
            "[1,63501] loss : 2.80585979\n",
            "[1,64001] loss : 2.87240192\n",
            "[1,64501] loss : 2.71990954\n",
            "[1,65001] loss : 2.73099990\n",
            "Test Loss is 2.732397\n",
            "[1,65501] loss : 2.73654508\n",
            "[1,66001] loss : 2.90012781\n",
            "[1,66501] loss : 2.72545472\n",
            "[1,67001] loss : 2.84467604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJkK61sRB45E"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.pooling import MaxPool2d\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import numpy as np\n",
        "# 必要的一些库\n",
        "import json\n",
        "N = 11\n",
        "inputc = 3\n",
        "batch = 4\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self,ic,oc,pad=1):\n",
        "    super().__init__()\n",
        "    self.conv=nn.Conv2d(\n",
        "        ic,oc,3,stride=1,padding=pad,dilation=1,\n",
        "        groups=1,bias=False,padding_mode='zeros'\n",
        "    )\n",
        "    self.bn = nn.BatchNorm2d(oc)\n",
        "    # 批量归一化\n",
        "  def forward(self,x):\n",
        "    y = self.conv(x)\n",
        "    y = self.bn(y)\n",
        "    y = torch.relu(y)\n",
        "    # relu激活函数\n",
        "    return y\n",
        "\n",
        "class RSN(nn.Module):\n",
        "  def __init__(self,ic,oc):\n",
        "      super().__init__()\n",
        "      self.conv_net = nn.Sequential(\n",
        "          CNN(ic,oc),\n",
        "          CNN(oc,ic)\n",
        "      )\n",
        "  def forward(self,x):\n",
        "    x = self.conv_net(x) + x\n",
        "    return x\n",
        "\n",
        "class outputhdv1(nn.Module):\n",
        "  def __init__(self,oc,hmc):\n",
        "    super().__init__()\n",
        "    self.cnn=CNN(oc,hmc)\n",
        "    self.pL = nn.Conv2d(hmc,1,1)\n",
        "  def forward(self,h):\n",
        "    x = self.cnn(h)\n",
        "    x = self.pL(x)\n",
        "    x = x.squeeze(1)\n",
        "    # with torch.autograd.set_detect_anomaly(True):\n",
        "    #   pl = torch.exp(x)\n",
        "    #   for i in range(batch):\n",
        "    #     pl[i] = pl[i] / pl[i].sum()\n",
        "    # 自定义softmax\n",
        "    # print(pl.shape)\n",
        "    # print(pl)\n",
        "    # nn.exit(0)\n",
        "    return x\n",
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self,b,f):\n",
        "    super().__init__()\n",
        "    self.name = \"res\"\n",
        "    self.size = (b,f)\n",
        "    self.iph = CNN(inputc,f*2,pad = 2)\n",
        "    self.pool1 = nn.MaxPool2d(3,stride = 1,padding = 0)\n",
        "    self.tru = nn.ModuleList()\n",
        "    self.tru.append(CNN(f*2,f))\n",
        "    for i in range(b):\n",
        "      self.tru.append(RSN(f,f))\n",
        "    # self.tru.append(nn.MaxPool2d(3,stride = 1,padding = 1))\n",
        "    # self.tru.append(CNN(f,f))\n",
        "    # for i in range(b):\n",
        "    #   self.tru.append(RSN(f,f))\n",
        "    # self.pool2 = (nn.AvgPool2d(3,stride = 1,padding = 1))\n",
        "    self.opt = outputhdv1(f,f)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # if(x.shape[1]==2):\n",
        "    #   x = torch.cat((x,torch.zeros(batch,1,N,N)),dim=1).to(device)\n",
        "    # print(x.device)\n",
        "    # print(x.shape)\n",
        "    # print(x)\n",
        "    # exit(0)\n",
        "    h = self.iph(x)\n",
        "    h = self.pool1(h)\n",
        "    for blk in self.tru :\n",
        "      h = blk(h)\n",
        "    h = self.opt(h)\n",
        "    return h\n",
        "\n",
        "ModelDic = {\n",
        "    \"res\":model\n",
        "}\n",
        "\n",
        "class bd:\n",
        "  def __init__(self):\n",
        "    self.board = np.zeros(shape=(2,N,N))\n",
        "  def transpose(self):\n",
        "    self.board = self.board.swapaxes(1,2)\n",
        "  def play(self,x,y,color):\n",
        "    self.board[color,y,x]=1\n",
        "  def isL(self,x,y):\n",
        "    if(self.board[0,y,x] or self.board[1,y,x]):\n",
        "      return False\n",
        "    return True\n",
        "\n",
        "def loadM(filep):\n",
        "  if(filep=='stochastic'):\n",
        "    return 'stochastic'\n",
        "  model = torch.load(filep,map_location=torch.device('cpu'))\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBWgmU7FR2Ts"
      },
      "outputs": [],
      "source": [
        "# 写自定义dataset类\n",
        "# import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "N = 11\n",
        "T = 1\n",
        "Nt = 500\n",
        "Ntrc = 10\n",
        "\n",
        "class Mydataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,filep):\n",
        "        \n",
        "        file = open(filep,\"r\")\n",
        "        rcm = file.readlines()\n",
        "        L = len(rcm)\n",
        "        i = 0\n",
        "        self.mrc = []\n",
        "\n",
        "        while i < L:\n",
        "            board=bd()\n",
        "            p = int(rcm[i])\n",
        "            sm = rcm[i+1].split()\n",
        "            k = 0\n",
        "            # print(i)\n",
        "            win = int(rcm[i+2])\n",
        "            if p>=50 and p<=170: \n",
        "              #去掉部分极端数据 \n",
        "              while k < p:\n",
        "                  x = int(sm[k])\n",
        "                  y = int(sm[k+1])\n",
        "                  o = ((k//2) & 1)\n",
        "                  if o == win and (random.randint(0,9)>7 or (random.randint(0,9)>5 and k>0.7*p)):\n",
        "                    # 学习中局策略\n",
        "                      a = copy.deepcopy(board.board)\n",
        "                      a = a.astype(np.float32)\n",
        "                      tmp = np.zeros(shape=(N,N))\n",
        "                      tmp[x][y] = 1\n",
        "                      if o == 1:\n",
        "                          b = np.asarray([a[1],a[0]])\n",
        "                          a = b.swapaxes(1,2)\n",
        "                          tmp = tmp.swapaxes(0,1)\n",
        "                      a=np.insert(a,2,a[0]-a[1],axis=0)\n",
        "                      self.mrc.append((a,copy.deepcopy(tmp)))\n",
        "                  board.play(x,y,o)\n",
        "                  k += 2\n",
        "\n",
        "            i += 3\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.mrc[index]\n",
        "    def __len__(self):\n",
        "        return len(self.mrc)\n",
        "\n",
        "def modeltest(model):\n",
        "    model.eval()\n",
        "    sumloss = 0\n",
        "    for input,label in test_loader:\n",
        "      input = input.to(device)\n",
        "      label = label.to(device)\n",
        "      if(label.size()[0]!=batch):\n",
        "          break      \n",
        "      with torch.no_grad():\n",
        "        outputs = model(input)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      loss = criterion(outputs, label)\n",
        "      sumloss += loss.item()\n",
        "    print(f\"Test Loss is {sumloss/(test_size)*batch:.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    train_data = Mydataset(\"TrainData.in2\")\n",
        "    test_data = Mydataset(\"TestData.in\")\n",
        "\n",
        "    train_size = len(train_data)\n",
        "    test_size = len(test_data)\n",
        "    # train_size = int(0.94 * len(dataset))\n",
        "    # test_size = len(dataset) - train_size\n",
        "    # train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size = batch,shuffle = True)\n",
        "    test_loader = DataLoader(test_data, batch_size= batch, shuffle = False)\n",
        "\n",
        "    mymod = loadM(\"/content/model_0_105000.pth\")\n",
        "    # mymod = model(9,128)\n",
        "    mymod = mymod.to(device)\n",
        "\n",
        "    print(train_size//batch,test_size//batch)\n",
        "    print(mymod.parameters)\n",
        "    # nn.exit()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.ASGD(mymod.parameters(),lr=0.001)\n",
        "    epochn = 6\n",
        "    \n",
        "    for epoch in range(epochn):\n",
        "        runningloss = 0.0\n",
        "        for i,data in enumerate(train_loader,0):\n",
        "            input,label = data\n",
        "\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            # print(input.device,label.device)\n",
        "\n",
        "            if(label.size()[0]!=batch):\n",
        "              break\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = mymod(input)\n",
        "            loss = criterion(output,label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            runningloss += loss.item()\n",
        "            if(i % Nt == 0 and i != 0):\n",
        "                print('[%d,%5d] loss : %.8f' %\n",
        "                        (epoch+1,i+1,runningloss/Nt))\n",
        "                if(i % (Nt*Ntrc) == 0):\n",
        "                    torch.save(mymod, f\"NR4/model_{epoch}_{i}.pth\")\n",
        "                    modeltest(mymod)\n",
        "                runningloss = 0.0\n",
        "                \n",
        "    print(\"Finished Training\")\n",
        "\n",
        "# 24000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "几次挺失败的训练，参数对应是\n",
        "\n",
        "6块，72深度或者42深度，加avgPool，最后误差率只能降低到0.083左右\n",
        "\n",
        "当然每个都只跑到两个epoch...\n",
        "\n",
        "好家伙，15块，10深度，只能跑到0.089左右的，还是跑了几乎3个epoch，每次都换优化方法跑\n",
        "\n",
        "真的不如之前121通道4块的时候。。。能跑到0.69，简直是传奇，深度很重要啊（虽然数据集不一样，那时MCTS数据多）\n",
        "\n",
        "6块128通道，有Maxpool带头，能跑到0.063，之后会过拟合。测试集降到0.062不下去了。\n",
        "\n",
        "调整了优化方法（降低学习率），额外降低了0.005～0.006左右,差不多能到0.06以下了\n",
        "\n",
        "谢谢，预测胜率完全预测不了，谢谢大家\n",
        "\n"
      ],
      "metadata": {
        "id": "QncleMnsaE3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 强化学习"
      ],
      "metadata": {
        "id": "VuaWFBrIvjQW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BP59aZp2rUBD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}